\appendix
\addappheadtotoc

\chapter{}

\section{How to Run Biohadoop}
\label{chap:usage:run}
The main purpose of Biohadoop is to be run in a Hadoop environment. Nevertheless,  it can also be run in a local environment. This is for example useful when new algorithms are developed. In this case the whole process of compilation, deployment to a Hadoop environment and testing can be abbreviated.

Three components must be available to run Biohadoop (four if Biohadoop is started in a Hadoop environment):

\begin{itemize}
  \item An installation of Java version 1.7 or higher. From here on it is assumed that Java is installed and configured and that the \texttt{JAVA\_HOME} environment variable points to this installation.
  \item All necessary libraries must be present and accessible in one or several folders.
  \item A valid configuration file must be present and accessible.
  \item The fourth component is only necessary when running Biohadoop in a Hadoop environment. This component is Hadoop itself which must be available in a version $\geq 2$.
\end{itemize}

Biohadoop can be started if those three components are provided (four for running on Hadoop). In a local environment, this is done by setting the right class paths when launching the program. In a Hadoop environment, the configuration option \texttt{includePaths} must be set to include the necessary files (see section \ref{chap:impl:configuration} on more information about how to configure Biohadoop). Those paths have to point to valid locations of an accessible HDFS file system.

Biohadoop was developed using Maven.\footnote{\url{https://maven.apache.org/} last access: 11.09.2014} Therefore, it is rather easy to get all the needed libraries, since they are declared as dependencies. The source code for Biohadoop can be found on GitHub: \url{https://github.com/gappc/biohadoop/}. By invoking the following command on the projects root folder, all dependencies are accumulated and put to the sub folder \texttt{target/dependency}:

\begin{lstlisting}[language=bash]
mvn dependency:copy-dependencies
\end{lstlisting}

From there, they can be directly referenced through Javas \texttt{classpath} option when running in a local environment. When running in a Hadoop environment, the libraries need to be copied to an accessible HDFS file system, and this location must be present in the \texttt{includePaths} configuration option mentioned above.
 
To use Biohadoop in a Hadoop environment, such an environment must be present. It can be a difficult task to configure such a Hadoop environment. Therefore, a pre-build Hadoop environment, developed during this thesis, is presented in appendix \ref{chap:appendix:biohadoop-docker}. The only dependency which this environment has is Docker.\footnote{\url{https://www.docker.com/} last access: 11.09.2014} Docker is a simple and lightweight runtime for virtual containers available on all major operating systems.

\subsection{Local Environment}
\label{chap:usage:local}
To start Biohadoop in a local environment, the class paths need to be set to include the necessary libraries. The necessary libraries can be obtained by invoking the Maven command, outlined above. The \texttt{-Dlocal} option must be provided to Java as an additional parameter. This is the only way to tell Biohadoop that it is launched in a local environment.

Lets assume, that all of the necessary libraries can be found at the location \path{/home/user/biohadoop/libs}, and the configuration file can be found at \path{/home/user/biohadoop/configs/simple-config-json}. Then, Biohadoop can be started in local mode by running the following command:
  
\begin{lstlisting}[language=bash]
java -Dlocal -cp /home/user/biohadoop/libs/* at.ac.uibk.dps.biohadoop.hadoop.BiohadoopApplicationMaster /home/user/biohadoop/configs/simple-config-json
\end{lstlisting}

A little bit hidden in the command we find the main class that starts Biohadoop, \texttt{at.ac.uibk.dps.biohadoop.hadoop.BiohadoopApplicationMaster}. This class takes care of starting the configured algorithms, endpoints and workers. After all of the algorithms have terminated, either because they have finished their computations or because of errors, Biohadoop shuts down.

When running Biohadoop in the local environment, all workers are started as threads in the same JVM as Biohadoop (in contrast to a Hadoop environment, where the workers are started in separate JVMs on perhaps different machines). This leads to the fact, that the workers have full access to all (static) objects of Biohadoop. But when those workers are started in a Hadoop environment, this access is not given anymore. So, one has to take care to rely only on the objects and properties that are provided to the used methods.

\subsection{Hadoop Environment}
\label{chap:usage:hadoop}
To start Biohadoop in a Hadoop environment (for example in the one provided in appendix \ref{chap:appendix:biohadoop-docker}), all needed libraries and configuration files must be present in the HDFS file system. In addition, Biohadoops jar file, i.e., the compiled library, must be accessible through the local file system, as it is started directly by Hadoop. The location of the libraries and configuration files in the HDFS file system must be configured in the configuration file that is provided to Biohadoop on startup.

Lets assume, that all of the necessary libraries can be found at the HDFS location \path{/biohadoop/libs}, the configuration file can be found at the HDFS location \path{/biohadoop/configs/simple-config-json} and that those paths are part of the configuration file that is provided to Biohadoop at startup. Furthermore, Biohadoops jar file can be found at \path{/home/user/biohadoop/biohadoop.jar}. Then, Biohadoop can be started in Hadoop mode by running the following command:

\begin{lstlisting}[language=bash]
yarn jar /home/user/biohadoop/biohadoop.jar at.ac.uibk.dps.biohadoop.hadoop.BiohadoopClient /biohadoop/configs/simple-config-json
\end{lstlisting}

As one can see, now the command \texttt{yarn} is used to launch Biohadoop. This command takes care of setting all the needed Hadoop environment variables, after which it starts the provided main class. The \texttt{yarn} command is part of Hadoop since version 2 and should be available if Hadoop is configured correctly.

In contrast to running Biohadoop in local mode, a different main class is launched. This is due to the fact that Yarn needs a startup class from where it loads the main program. By looking at the source code of \texttt{at.ac.uibk.dps.biohadoop.hadoop.BiohadoopClient}, one will notice that it starts the \texttt{BiohadoopApplicationMaster} behind the curtains (\texttt{BiohadoopApplicationMaster} is the class that is directly started in local mode).

When running Biohadoop in a Hadoop environment, all workers are started in their own containers, which are under the control of Hadoop. The result is, that the workers can not access the (static) objects of Biohadoop. If one wants to access some properties of Biohadoop, this must be done through the provided communication facilities of Biohadoop. It is, nevertheless, possible to implement different communication facilities if this is needed.

\section{Pre-build Hadoop Environment using Docker}
\label{chap:appendix:biohadoop-docker}
This section provides a method to run a pre-configured Hadoop environment using Docker. Docker uses its so called Dockerfiles for its configuration. The solution presented here installs Apache Hadoop 2.5.0, Apache Oozie 4.0.1 and Apache ZooKeeper 3.4.6 as a cluster environment.

\subsection{Build the Hadoop Environment}
The following commands are used to clone the repository to the current local directory and to build the Docker image. Be aware that during the cloning process about \unit[400]{MB} of data is transferred. This is due to Oozie, which is delivered in a pre-compiled form.
\begin{lstlisting}[language=bash]
git clone https://github.com/gappc/docker-biohadoop.git
cd docker-biohadoop
sudo docker build -t="docker-biohadoop" .
\end{lstlisting}

The project \texttt{docker-biohadoop} provides two scripts, located in the scripts directory, that can be used to start and stop \texttt{docker-biohadoop} instances. Those scripts need to be executable:

\begin{lstlisting}[language=bash]
chmod +x scripts/*.sh
\end{lstlisting}

\subsection{Running Hadoop}
After that we are able to start Hadoop instances by using the first script: \texttt{docker-run-hadoop.sh}. This script starts a number of Hadoop instances. It takes the number of slaves (nr-of-slaves) as argument. The script starts one Docker container as Hadoop master and additional nr-of-slaves Docker containers as Hadoop slaves. For example, one Hadoop master instance and two slaves are started by the following command:

\begin{lstlisting}[language=bash]
scripts/docker-run-hadoop.sh 2
\end{lstlisting}

Note that also the master is used for computational purposes. Therefor, Hadoop has 3 machines for computation with the settings above.

After invoking \texttt{docker-run-hadoop.sh}, a gnome-terminal is started for every Docker container. The master containers terminal has a red color, the slaves terminals are yellow. The master container starts the Hadoop environment, which may take some time (depending on the hardware and the number of slaves). After this initialization, the Hadoop cluster is ready for usage. Try to invoke the command \texttt{jps} on all running containers to look if Hadoop is running:

\begin{lstlisting}[language=bash]
jps
\end{lstlisting}

On the master node, it should output:
\begin{itemize}
  \item DataNode
  \item JobHistoryServer
  \item NodeManager
  \item NameNode
  \item QuorumPeerMain
  \item ResourceManager
  \item SecondaryNameNode
\end{itemize}

On the slave nodes it should output:
\begin{itemize}
  \item DataNode
  \item NodeManager
\end{itemize}

\subsection{Stopping Hadoop}
By using the following command, all running Docker containers are forcefully stopped and their interfaces are removed from the host. It is no problem to forcefully stop Docker containers, as they don't keep any state by default.
\begin{lstlisting}[language=bash]
scripts/docker-stop-all.sh
\end{lstlisting}

\subsection{SSH access}
The master node is accessible with user root, a password is generated on each startup and printed on the master terminal. Consider adding your SSH key to the Dockerfile if you are going to use \texttt{docker-biohadoop} often.