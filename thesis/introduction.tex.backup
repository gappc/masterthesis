\chapter{Introduction}
\label{chap:introduction}
Bio-inspired optimization algorithms are used to find good (preferably best) solutions to a given optimization problem. They mimic in an abstract way the behavior of biological agents. A prominent example is the genetical algorithm (GA) that uses a simplified model of natural evolution to improve the solutions for an optimization problem. Another example is the particle swarm optimization (PSO) that is based on the movement of bird flocks.

Most optimization problems are NP-hard. It is not feasible to explore all possible solutions as this would take to much time. Bio-inspired optimization algorithms can't change this fundamental issue, but in many cases their approximation techniques provide ``good enough'' solutions to a problem in an acceptable time.

Parallelization techniques can be used to further reduce the computational time or to search through more candidate solutions in the same amount of time. There are different approaches to parallelization on current computer architectures. Some of them focus on the exploitation of processing units that are local to a given machine like SIMD, multi core or specialized hardware (GPU, FPGA, ASIC). This local approaches work well in many cases, but the increase of computational power is limited to the amount of resources that single machine can handle.

Further performance increases can only be achieved by distributing the work to several machines, forming a cluster. The downside of this approach is the increased complexity. On the one side, the machines and their components (e.g. hard drives) are unreliable and may fail. This must be detected and handled in an appropriate way. On the other side, the machines need to communicate to each other through the network to exchange work tasks and results.

Apache Hadoop is an open source software project that provides management tools for clusters and libraries to build distributed applications. It is often referred to as an operating system for clusters. It assumes that cluster hardware is inherently unreliable and provides mechanisms to automatically detect and handle failures. This allows distributed applications to focus on the implementation, rather than on cluster management concerns. Hadoop also provides the mechanisms to start, stop and monitor the distributed applications, automatically restarting them if needed.

Hadoop versions prior to 2.0 were restricted to the MapReduce computational model. This restriction made it difficult to implement e.g. iterative algorithms, like the bio-inspired optimization techniques. The release of Hadoop 2.0 changed the resource management implementation to YARN which makes no assumptions about the executed application.

One drawback of YARN is its lack of support for application specific communication. This restriction comes by design, as YARNs purpose is to manage the cluster, its resources and the running applications.

Different projects try to solve the communication issue. Apache Spark for example implements a stream processing model on top of Hadoop, where cluster nodes are connected to form a graph through which the data flows. Data processing and transformation is performed on the nodes. Apache Spark is another implementation that supports the stream model and a batch processing model on top of Hadoop. In addition, it provides a distributed in-memory store.

Biohadoop is another project that aims to simplify the implementation of distributed applications on top of Hadoop. It was developed during this thesis and works based on the master - worker pattern. Biohadoop offers an abstract communication mechanism that makes it easy to distribute work items from the master node to any number of worker nodes. Its focus on the master - worker pattern makes it more lightweight than the previous mentioned solutions, but has the drawback to be restricted to the mentioned pattern.

This thesis introduces Biohadoop and demonstrates its usefulness by implementing two bio-inspired optimization techniques on top of it.

% The parallelization of algorithms can be a hard task, where knowledge and correct application of the available tools is a crucial factor for success. Beside the fact, that not every programmer has that kind of knowledge, it is an error prone process. So it is useful, if the low level parts of parallelization are hidden behind an API.
% 
% There are several approaches in parallelization. All of them boil down to distribute the computational work to different resources like CPU or GPU. The resources can reside on a local system, but also on remote computers. On local resources, processes and threads are common abstractions for parallel programs. They are sometimes hidden by higher level API's like OpenMP \cite{dagum1998openmp}\cite{chapman2008using}.
% 
% For the distribution and computation on remote systems, other solutions exist. One of the most popular is MPI \cite{mpi}, that uses message passing for the communication between its computational entities. Alternatives, like Global Arrays \cite{nieplocha1994global} and their successors, the partitioned global address space languages \cite{coarfa2005evaluation}, try to provide a global address space over a distributed system. All those approaches have their advantages, like good portability and high performance, but they are often applied only in educational institutes or by the scientific community, because they have one main use case: high performance computation (HPC). HPC is for example used in simulations and machine learning. Only a few institutes and companies are interested to spend money for the hard- and software just for this purpose.
% 
% In contrast to the specialized solutions mentioned above, Apache Hadoop \cite{hadoop} provides an open source solution that continues to gain support in education, science and business. It has no special hardware needs and can run on a cluster of commodity hardware.
% 
% In the beginning, Hadoop was restricted to the MapReduce \cite{dean2008mapreduce} programming model, that is well suited for parallel data analysis on a big scale. In this sense, it is often compared to database systems \cite{pavlo2009comparison}\cite{stonebraker2010mapreduce}\cite{floratou2012can} or combined with them  \cite{abouzeid2009hadoopdb}\cite{su2012oracle}. Other projects extend MapReduce to provide SQL-like capabilities. Pig  \cite{gates2009building} is a high-level dataflow system and provides an SQL-style language (called Pig Latin), that is compiled to MapReduce jobs. Hive \cite{thusoo2010hive} is a data warehousing solution, that supports queries expressed in HiveQL, also a language similar to SQL. The analytical capabilities make MapReduce on Hadoop an interesting choice for many companies.
% 
% %!!!SEE GRAYSORT for performance examples, 8. October 2014 new deadline!!!
% 
% % http://www.cs.arizona.edu/~bkmoon/papers/sigmodrec11.pdf, page 4 for more information
% But MapReduce usually performs poor, when it is applied to problems that don't fit into its scheme \cite{ekanayake2008mapreduce}\cite{bu2010haloop}\cite{rosen2013iterative}\cite{ye2009stochastic}. An outstanding example for such problems is the category of iterative algorithms, Bio-inspired optimization techniques belong to them. There are approaches to expand MapReduces capabilities, like HaLoop \cite{bu2010haloop}, Pregrel \cite{malewicz2010pregel} and Twister \cite{ekanayake2010twister}. All of them try to use MapReduce for purposes it wasn't designed for - with varying degrees of success.
% 
% Since the upcoming of YARN \cite{vavilapalli2013apache}, the next generation of Hadoop's compute platform, the programming model for Hadoop has moved its focus from the analytical model, to a general purpose execution style. YARN is expressive enough to cover MapReduce, which is still an important part of Hadoop. But it can also be adopted for other computational styles, like the iterative one.
% 
% YARN is relatively new, the first general available and supported release was for Apache Hadoop 2.2 at October 2013 \cite{hadoop-2.2.0}. Because of that, just a few frameworks support parallel iterative applications on YARN. Apache Spark \cite{spark}\cite{zaharia2012resilient} uses resilient distributed datasets (RDDs) \cite{zaharia2012resilient}. RDDs are basically immutable collections, on which defined operations can be performed (e.g. map, filter and join). Because RDDs are stored in memory, it is really fast to work with them. But as the number of available operations is limited, Spark is not suitable for all kinds of computations. Spark is the foundation for other projects like Apache Mahout \cite{mahout} (beginning with version 1.0), which is a machine learning library.
% 
% Another framework, that has gained a lot of attention, but doesn't use YARN as its resource manager, is Apache Storm \cite{storm}. Storm is typically used to process unbounded streams of data, although it can be used for a broad range of workloads. The streams are send through a configurable graph, where each node does some processing on the data. Apache Tez \cite{tez} and Apache Samza \cite{samza} are similar to Storm, but run on YARN.
% 
% %In contrast to the frameworks mentioned above, Apache Twill \cite{twill} and Spring for Apache Hadoop \cite{spring-hadoop} are libraries. They support a programmer in writing programs, that run on YARN. The whole program architecture and communication must be done manually.
% 
% Biohadoop, the program developed during this master thesis, is an alternative to the solutions presented above. It is a framework, that runs on top of YARN and is specifically designed to run iterative algorithms, for example Bio-inspired optimization algorithms. This is achieved by using a master - worker approach, where computationally intense work is distributed from the master to the workers. The data transmission between the master and its workers is configurable, so it can fit to the problem and environment. In addition, by providing a simple asynchronous API, the details of algorithm parallelization are hidden from the user.
% 
% Compared to the other solutions, Biohadoop is also capable to distribute it's tasks to external workers, that are not under the control of Hadoop, like web browsers or mobile phones. Through this mechanism, it is possible to harness additional computational capacities. Possible usage scenarios are distributed computing projects, like Folding@home \cite{foldingathome} or SETI@home \cite{setiathome}.
% 
% In addition to Biohadoop, an extension to Apache Oozie \cite{islam2012oozie}\cite{oozie} was implemented. Oozie is a workflow management system for Hadoop. Through this extension, Oozie is able to include Biohadoop in its workflows. The workflows are useful, if several steps are needed to complete a bigger task, for example the simulation of a protein and analysis of the results afterwards.

The rest of the document is organized as follows: chapter \ref{chap:bioalgorithms} provides an introduction to bio-inspired optimization algorithms as well as an overview of two common representatives: GA and PSO. Chapter \ref{chap:hadoop} delivers information about Apache Hadoop and Oozie that is needed to understand the functionality of Biohadoop. Chapter \ref{chap:impl} explains Biohadoops architecture and the implemented modifications for Oozie (section \ref{chap:impl:oozie}). Chapter \ref{chap:evaluation} evaluates the performance of Biohadoop using two different implementations of a GA. The conclusions in chapter \ref{chap:conclusions} summarize the master thesis and the obtained results.

% The appendix contains additional material, that completes the information given in the prior chapters. It is also there, where information about a pre-build environment for Hadoop and Oozie can be found, together with a quickstart guide on how to run Biohadoop on this environment.

% The bio-inspired algorithms are used as examples of a class of problems, that can be efficiently solved in a distributed manner by Biohadoop

%  Chapter \ref{chap:usage} shows, how Biohadoop's API can be used to implement new algorithms. It also demonstrates the steps necessary to execute Biohadoop.