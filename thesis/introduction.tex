\chapter{Introduction}
Bio-inspired optimization algorithms are used to find good (preferably best) solutions to a given optimization problem. They mimic the behavior of biological agents. A prominent example is the genetic algorithm (GA) \cite{sivanandam2008genetic} that uses a simplified model of natural evolution to improve the solutions towards an optimum. Another example is the particle swarm optimization (PSO) \cite{kennedy2010particle}, which is based on the movement of bird flocks.

Most optimization problems are NP-hard. It is not feasible to explore all possible solutions, since this would simply take too much time. Bio-inspired optimization algorithms can't change this fundamental issue, however, for many problems their approximation techniques provide solutions which are ``good enough'' in an acceptable time.

% Parallelization techniques can be used to further reduce the computational time or to search through more candidate solutions in the same amount of time.
Parallelization techniques can be applied to further reduce computational time by parallelizing a single sample evaluation or by evaluating more samples in parallel. There are different approaches to parallelization on current computer architectures. Some of them focus on the exploitation of processing units that are local to a given machine like SIMD\footnote{\url{http://en.wikipedia.org/wiki/SIMD} last access: 07.01.2015}, multi core\footnote{\url{https://en.wikipedia.org/wiki/Multi-core_processor} last access: 07.01.2015} or specialized hardware (GPU\footnote{\url{http://en.wikipedia.org/wiki/Graphics_processing_unit} last access: 07.01.2015}, FPGA\footnote{\url{https://en.wikipedia.org/wiki/Field-programmable_gate_array} last access: 07.01.2015}, ASIC\footnote{\url{https://en.wikipedia.org/wiki/Application-specific_integrated_circuit} last access: 07.01.2015}). Local approaches work well in many cases, but the increase of computational power is limited by the resources of a single computer.

Further performance increases can be achieved by distributing the work among several computers, forming a cluster. The downside of this approach is the increased overall complexity. On the one side, hardware components are unreliable and may fail (e.g. hard drives) are unreliable and may fail which must be detected and handled in an appropriate way. On the other side, the computers need to communicate with each other through the network to distribute work tasks and data.

Apache Hadoop \footnote{\url{http://hadoop.apache.org/} last access: 07.01.2015} is an open source software project that provides management tools for clusters and libraries to build distributed applications. It is often referred to as an operating system for clusters. It assumes that cluster hardware is inherently unreliable and provides mechanisms to automatically detect and handle failures. This enables distributed applications to focus on the implementation rather than on cluster management concerns. Hadoop also provides the mechanisms to start, stop and monitor distributed applications and automatically restart them if needed.

Hadoop versions prior to 2.0 were restricted to the MapReduce \cite{dean2008mapreduce} computational model. This restriction made it difficult to implement, e.g., iterative algorithms, like the bio-inspired optimization techniques. With the release of Hadoop 2.0 the resource management implementation has been changed to YARN \cite{vavilapalli2013apache} which makes no assumptions about the application being executed .

One drawback of YARN, however, is the missing support for application specific communication. This restriction comes by design, as YARNs purpose is to manage the cluster, its resources and the running applications.

Different software projects try to improve Hadoop and solve the communication issue. Apache Storm \footnote{\url{https://storm.apache.org/} last access: 01.10.2014} implements a stream processing model on top of Hadoop, where cluster nodes are connected to form a graph through which the data flows. Data processing and transformation is performed on the nodes. Apache Spark \footnote{\url{https://spark.apache.org/} last access: 01.10.2014} supports both the stream model and a batch processing model on top of Hadoop. In addition, it provides a distributed in-memory store \cite{zaharia2012resilient}.

Biohadoop \footnote{\url{https://github.com/gappc/biohadoop/} last access: 15.12.2014} is another project whose goal is to simplify the implementation of distributed applications on top of Hadoop. It was developed during this thesis and works based on the master-worker pattern. Biohadoop offers an abstract communication mechanism that makes it easy to distribute work items from the master node to any number of worker nodes. The focus on the master-worker pattern makes it more lightweight than the previous solutions mentioned above, provided that the problem fits into the master-worker pattern.

In this thesis, Biohadoop is introduced and its usefulness is demonstrated with two bio-inspired optimization techniques. It provides additional information about Apache Oozie \footnote{\url{https://oozie.apache.org/} last access: 01.10.2014} (a Hadoop workflow tool) and how it was extended to support Biohadoop.

% The parallelization of algorithms can be a hard task, where knowledge and correct application of the available tools is a crucial factor for success. Beside the fact, that not every programmer has that kind of knowledge, it is an error prone process. So it is useful, if the low level parts of parallelization are hidden behind an API.
% 
% There are several approaches in parallelization. All of them boil down to distribute the computational work to different resources like CPU or GPU. The resources can reside on a local system, but also on remote computers. On local resources, processes and threads are common abstractions for parallel programs. They are sometimes hidden by higher level API's like OpenMP \cite{dagum1998openmp}\cite{chapman2008using}.
% 
% For the distribution and computation on remote systems, other solutions exist. One of the most popular is MPI \cite{mpi}, that uses message passing for the communication between its computational entities. Alternatives, like Global Arrays \cite{nieplocha1994global} and their successors, the partitioned global address space languages \cite{coarfa2005evaluation}, try to provide a global address space over a distributed system. All those approaches have their advantages, like good portability and high performance, but they are often applied only in educational institutes or by the scientific community, because they have one main use case: high performance computation (HPC). HPC is for example used in simulations and machine learning. Only a few institutes and companies are interested to spend money for the hard- and software just for this purpose.
% 
% In contrast to the specialized solutions mentioned above, Apache Hadoop \cite{hadoop} provides an open source solution that continues to gain support in education, science and business. It has no special hardware needs and can run on a cluster of commodity hardware.
% 
% In the beginning, Hadoop was restricted to the MapReduce \cite{dean2008mapreduce} programming model, that is well suited for parallel data analysis on a big scale. In this sense, it is often compared to database systems \cite{pavlo2009comparison}\cite{stonebraker2010mapreduce}\cite{floratou2012can} or combined with them  \cite{abouzeid2009hadoopdb}\cite{su2012oracle}. Other projects extend MapReduce to provide SQL-like capabilities. Pig  \cite{gates2009building} is a high-level dataflow system and provides an SQL-style language (called Pig Latin), that is compiled to MapReduce jobs. Hive \cite{thusoo2010hive} is a data warehousing solution, that supports queries expressed in HiveQL, also a language similar to SQL. The analytical capabilities make MapReduce on Hadoop an interesting choice for many companies.
% 
% %!!!SEE GRAYSORT for performance examples, 8. October 2014 new deadline!!!
% 
% % http://www.cs.arizona.edu/~bkmoon/papers/sigmodrec11.pdf, page 4 for more information
% But MapReduce usually performs poor, when it is applied to problems that don't fit into its scheme \cite{ekanayake2008mapreduce}\cite{bu2010haloop}\cite{rosen2013iterative}\cite{ye2009stochastic}. An outstanding example for such problems is the category of iterative algorithms, Bio-inspired optimization techniques belong to them. There are approaches to expand MapReduces capabilities, like HaLoop \cite{bu2010haloop}, Pregrel \cite{malewicz2010pregel} and Twister \cite{ekanayake2010twister}. All of them try to use MapReduce for purposes it wasn't designed for - with varying degrees of success.
% 
% Since the upcoming of YARN \cite{vavilapalli2013apache}, the next generation of Hadoop's compute platform, the programming model for Hadoop has moved its focus from the analytical model, to a general purpose execution style. YARN is expressive enough to cover MapReduce, which is still an important part of Hadoop. But it can also be adopted for other computational styles, like the iterative one.
% 
% YARN is relatively new, the first general available and supported release was for Apache Hadoop 2.2 at October 2013 \cite{hadoop-2.2.0}. Because of that, just a few frameworks support parallel iterative applications on YARN. Apache Spark \cite{spark}\cite{zaharia2012resilient} uses resilient distributed datasets (RDDs) \cite{zaharia2012resilient}. RDDs are basically immutable collections, on which defined operations can be performed (e.g. map, filter and join). Because RDDs are stored in memory, it is really fast to work with them. But as the number of available operations is limited, Spark is not suitable for all kinds of computations. Spark is the foundation for other projects like Apache Mahout \cite{mahout} (beginning with version 1.0), which is a machine learning library.
% 
% Another framework, that has gained a lot of attention, but doesn't use YARN as its resource manager, is Apache Storm \cite{storm}. Storm is typically used to process unbounded streams of data, although it can be used for a broad range of workloads. The streams are send through a configurable graph, where each node does some processing on the data. Apache Tez \cite{tez} and Apache Samza \cite{samza} are similar to Storm, but run on YARN.
% 
% %In contrast to the frameworks mentioned above, Apache Twill \cite{twill} and Spring for Apache Hadoop \cite{spring-hadoop} are libraries. They support a programmer in writing programs, that run on YARN. The whole program architecture and communication must be done manually.
% 
% Biohadoop, the program developed during this master thesis, is an alternative to the solutions presented above. It is a framework, that runs on top of YARN and is specifically designed to run iterative algorithms, for example Bio-inspired optimization algorithms. This is achieved by using a master-worker approach, where computationally intense work is distributed from the master to the workers. The data transmission between the master and its workers is configurable, so it can fit to the problem and environment. In addition, by providing a simple asynchronous API, the details of algorithm parallelization are hidden from the user.
% 
% Compared to the other solutions, Biohadoop is also capable to distribute it's tasks to external workers, that are not under the control of Hadoop, like web browsers or mobile phones. Through this mechanism, it is possible to harness additional computational capacities. Possible usage scenarios are distributed computing projects, like Folding@home \cite{foldingathome} or SETI@home \cite{setiathome}.
% 
% In addition to Biohadoop, an extension to Apache Oozie \cite{islam2012oozie}\cite{oozie} was implemented. Oozie is a workflow management system for Hadoop. Through this extension, Oozie is able to include Biohadoop in its workflows. The workflows are useful, if several steps are needed to complete a bigger task, for example the simulation of a protein and analysis of the results afterwards.

The rest of the document is organized as follows: chapter \ref{chap:bioalgorithms} provides an introduction to bio-inspired optimization algorithms as well as an overview of two common representatives: GA and PSO. Chapter \ref{chap:hadoop} provides information about Hadoop and Oozie needed to understand the functionality of Biohadoop. Chapter \ref{chap:impl} explains Biohadoops architecture and the modifications implemented for Oozie (section \ref{chap:impl:oozie}). Chapter \ref{chap:evaluation} evaluates the performance of Biohadoop using two different implementations of a GA. The conclusions in chapter \ref{chap:conclusions} summarize the master thesis and the obtained results.

% The appendix contains additional material, that completes the information given in the prior chapters. It is also there, where information about a pre-build environment for Hadoop and Oozie can be found, together with a quickstart guide on how to run Biohadoop on this environment.

% The bio-inspired algorithms are used as examples of a class of problems, that can be efficiently solved in a distributed manner by Biohadoop

%  Chapter \ref{chap:usage} shows, how Biohadoop's API can be used to implement new algorithms. It also demonstrates the steps necessary to execute Biohadoop.