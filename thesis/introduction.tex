\chapter{Introduction}
\label{chap:introduction}
The parallelization of algorithms can be a hard task, where knowledge and correct application of the available tools is a crucial factor for success. Beside the fact, that not every programmer has that kind of knowledge, it is an error prone process. So it is useful, if the low level parts of parallelization are hidden behind an API.

There are several approaches in parallelization. All of them boil down to distribute the computational work to different resources like CPU or GPU. The resources can reside on a local system, but also on remote computers. On local resources, processes and threads are common abstractions for parallel programs. They are sometimes hidden by higher level API's like OpenMP \cite{dagum1998openmp}\cite{chapman2008using}.

For the distribution and computation on remote systems, other solutions exist. One of the most popular is MPI \cite{mpi}, that uses message passing for the communication between its computational entities. Alternatives, like Global Arrays \cite{nieplocha1994global} and their successors, the partitioned global address space languages \cite{coarfa2005evaluation}, try to provide a global address space over a distributed system. All those approaches have their advantages, like good portability and high performance, but they are often applied only in educational institutes or by the scientific community, because they have one main use case: high performance computation (HPC). HPC is for example used in simulations and machine learning. Only a few institutes and companies are interested to spend money for the hard- and software just for this purpose.

In contrast to the specialized solutions mentioned above, Apache Hadoop \cite{hadoop} provides an open source solution that continues to gain support in education, science and business. It has no special hardware needs and can run on a cluster of commodity hardware.

In the beginning, Hadoop was restricted to the MapReduce \cite{dean2008mapreduce} programming model, that is well suited for parallel data analysis on a big scale. In this sense, it is often compared to database systems \cite{pavlo2009comparison}\cite{stonebraker2010mapreduce}\cite{floratou2012can} or combined with them  \cite{abouzeid2009hadoopdb}\cite{su2012oracle}. Other projects extend MapReduce to provide SQL-like capabilities. Pig  \cite{gates2009building} is a high-level dataflow system and provides an SQL-style language (called Pig Latin), that is compiled to MapReduce jobs. Hive \cite{thusoo2010hive} is a data warehousing solution, that supports queries expressed in HiveQL, also a language similar to SQL. The analytical capabilities make MapReduce on Hadoop an interesting choice for many companies.

%!!!SEE GRAYSORT for performance examples, 8. October 2014 new deadline!!!

% http://www.cs.arizona.edu/~bkmoon/papers/sigmodrec11.pdf, page 4 for more information
But MapReduce usually performs poor, when it is applied to problems that don't fit into its scheme \cite{ekanayake2008mapreduce}\cite{bu2010haloop}\cite{rosen2013iterative}\cite{ye2009stochastic}. An outstanding example for such problems is the category of iterative algorithms, Bio-inspired optimization techniques belong to them. There are approaches to expand MapReduces capabilities, like HaLoop \cite{bu2010haloop}, Pregrel \cite{malewicz2010pregel} and Twister \cite{ekanayake2010twister}. All of them try to use MapReduce for purposes it wasn't designed for - with varying degrees of success.

Since the upcoming of YARN \cite{vavilapalli2013apache}, the next generation of Hadoop's compute platform, the programming model for Hadoop has moved its focus from the analytical model, to a general purpose execution style. YARN is expressive enough to cover MapReduce, which is still an important part of Hadoop. But it can also be adopted for other computational styles, like the iterative one.

YARN is relatively new, the first general available and supported release was for Apache Hadoop 2.2 at October 2013 \cite{hadoop-2.2.0}. Because of that, just a few frameworks support parallel iterative applications on YARN. Apache Spark \cite{spark}\cite{zaharia2012resilient} uses resilient distributed datasets (RDDs) \cite{zaharia2012resilient}. RDDs are basically immutable collections, on which defined operations can be performed (e.g. map, filter and join). Because RDDs are stored in memory, it is really fast to work with them. But as the number of available operations is limited, Spark is not suitable for all kinds of computations. Spark is the foundation for other projects like Apache Mahout \cite{mahout} (beginning with version 1.0), which is a machine learning library.

Another framework, that has gained a lot of attention, but doesn't use YARN as its resource manager, is Apache Storm \cite{storm}. Storm is typically used to process unbounded streams of data, although it can be used for a broad range of workloads. The streams are send through a configurable graph, where each node does some processing on the data. Apache Tez \cite{tez} and Apache Samza \cite{samza} are similar to Storm, but run on YARN.

%In contrast to the frameworks mentioned above, Apache Twill \cite{twill} and Spring for Apache Hadoop \cite{spring-hadoop} are libraries. They support a programmer in writing programs, that run on YARN. The whole program architecture and communication must be done manually.

Biohadoop, the program developed during this master thesis, is an alternative to the solutions presented above. It is a framework, that runs on top of YARN and is specifically designed to run iterative algorithms, for example Bio-inspired optimization algorithms. This is achieved by using a master - worker approach, where computationally intense work is distributed from the master to the workers. The data transmission between the master and its workers is configurable, so it can fit to the problem and environment. In addition, by providing a simple asynchronous API, the details of algorithm parallelization are hidden from the user.

Compared to the other solutions, Biohadoop is also capable to distribute it's tasks to external workers, that are not under the control of Hadoop, like web browsers or mobile phones. Through this mechanism, it is possible to harness additional computational capacities. Possible usage scenarios are distributed computing projects, like Folding@home \cite{foldingathome} or SETI@home \cite{setiathome}.

In addition to Biohadoop, an extension to Apache Oozie \cite{islam2012oozie}\cite{oozie} was implemented. Oozie is a workflow management system for Hadoop. Through this extension, Oozie is able to include Biohadoop in its workflows. The workflows are useful, if several steps are needed to complete a bigger task, for example the simulation of a protein and analysis of the results afterwards.

The rest of the document is organized as follows: in chapter \ref{chap:bioalgorithms}, an introduction to Bio-inspired algorithms is given, as well as an overview of common representatives. The Bio-inspired algorithms are used as examples of a class of problems, that can be efficiently solved in a distributed manner by Biohadoop. Chapter \ref{chap:hadoop} provides the information about Apache Hadoop and Oozie, that is needed to understand the functionality of Biohadoop. Chapter \ref{chap:impl} then introduces Biohadoop's architecture and its components, and shows the implemented modifications for Oozie in section \ref{chap:impl:oozie}. Chapter \ref{chap:usage} shows, how Biohadoop's API can be used to implement new algorithms. It also demonstrates the steps necessary to execute Biohadoop. Chapter \ref{chap:results} shows and compares the performance of Biohadoop, using its various communication facilities. The conclusions in chapter \ref{chap:conclusions} summarize the master thesis and the results obtained. The appendix contains additional material, that completes the information given in the prior chapters. It is also there, where information about a pre-build environment for Hadoop and Oozie can be found, together with a quickstart guide on how to run Biohadoop on this environment.