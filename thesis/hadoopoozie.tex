\chapter{Hadoop}
\label{chap:hadoop}
Apache Hadoop is an open source software project for massive data processing and parallel computing in a cluster. It derives from Google's publications about MapReduce \cite{dean2008mapreduce} and Google File System (GFS) \cite{ghemawat2003google}, but has undergone quite some changes due to its active development, e.g., the introduction of YARN (see section \ref{chap:hadoop:yarn}).

Hadoop is designed to run on commodity hardware and provides a high degree of fault tolerance, implemented in software. It scales from a single server up to thousands of machines. Each machine stores data and is used for computation.

In the following key properties of Hadoop are listed:
\begin{description}
  \item[Scalability]: Hadoop is able to scale horizontally, which means that the performance scales with the number of cluster machines. The performance can be improved by dynamically adding nodes at runtime if required. Furthermore, unnecessary nodes can be shut down if their additional performance is not needed.
  \item[Cost effectiveness]: Since Hadoop runs on commodity hardware there is no need for exclusive, specialized proprietary hardware. It is possible to run Hadoop on an existing infrastructure. Hadoop runs also in the cloud. Different offers from Amazon, Google, Cloudera, etc. exist. This can further reduce costs, as the customer has to pay only for the resources used and doesn't need to maintain its own data center.
  \item[Flexibility]: Hadoop can handle any kind of data. Custom applications running on Hadoop enable the transformation of, and computation on arbitrary data. For example, Hadoop can be used to analyze log files. It can equally well be used by physicists to find patterns in their huge datasets.
  \item[Fault tolerance]: Hadoop expects hardware failures and is designed from ground up with fault tolerance in mind. If a machine fails, Hadoop automatically redirects the computations and data of this machine to another machine.
\end{description}

Hadoop consists of two main components, HDFS (Hadoop Distributed File System) and YARN (Yet Another Resource Manager). HDFS is a distributed, scalable and reliable file system. YARN assigns resources (CPU, memory, and storage) to applications running on a Hadoop cluster and is part of Hadoop since version 2.0. It replaced the resource management capabilities, that were bundled in MapReduce prior to Hadoop version 2.0. Since this version, MapReduce uses YARN for the resource management.

YARNs concept of resources is more flexible than the MapReduce resource management model used for Hadoop prior to version 2.0. The old resource management is tailored to MapReduce tasks and divides the cluster resources into separate map and reduce slots. Map tasks and reduce tasks are only executed on map slots and reduce slots, respectively. The number of map and reduce slots are statically configured and established during the startup of Hadoop. This can lead to poor resource usage if the MapReduce applications don't fit to the configured number of slots. The situation gets even worse for applications with a different computational model like Biohadoop, which provides a framework for iterative computations. With the old resource management model iterative tasks run either on the map slots or on the reduce slots while unused slots idle. Therefore, resources are not fully exploited. YARN, on the other hand, is able to host arbitrary computational models, as it provides the resources in an application agnostic way. More details about YARNs resource management can be found in section \ref{chap:hadoop:yarn}.

Figure \ref{fig:hadoop-layer} shows the current architecture of Hadoop. HDFS provides data services as the basic layer. YARN builds on top of HDFS and manages the resources of the cluster. The different applications, like MapReduce, Storm, Spark, Biohadoop, etc. run on top of YARN.

\begin{figure}
  \centering
  \includegraphics[width=130mm,natwidth=789,natheight=161]{hadoop-layer.png}
  \caption[Hadoop layers]{Hadoop layers: HDFS forms the base and provides data services, YARN builds on it and provides resource management. The applications run on top of YARN.}
  \label{fig:hadoop-layer}
\end{figure}

\section{HDFS}
\label{chap:hadoop:hdfs}
HDFS is the distributed, scalable and reliable file system of Hadoop, written in Java. A HDFS cluster consists of a single NameNode and several DataNodes. The NameNode manages the file system namespace, regulates the client access to files, commands the DataNodes and decides where to store the data. A DataNode stores the assigned data on the storage that is attached to its cluster node (usually there is one DataNode running on each cluster node). HDFS performs file transfer and storage only between clients and DataNodes or among DataNodes. The NameNode never comes directly in touch with the user data. This way, HDFS can scale by adding more DataNodes.

The files in HDFS are stored in blocks of a configurable maximum size (default \unit[128]{MB}). Reliability arises from the replication of blocks to other available DataNodes. The decision where to store data replicas is made by the NameNode, the exchange of replicas is performed directly between the DataNodes. The number of replicas is configurable and has a default value of 3, which means that each block is stored three times in HDFS. If a node goes down, for example because of a hardware failure, HDFS automatically replicates this node's data blocks to other nodes by using the remaining copies, such that the replication factor is satisfied again. Files that are bigger than the maximum block size are split into several smaller blocks. The resulting blocks are handled as described above.

The restriction of a single NameNode instance makes the NameNode effectively a single point of failure, but there exist solutions for high availability that make use of a Quorum Journal manager\footnote{\url{https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html} last access: 12.11.2014} or NFS.\footnote{\url{https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html} last access: 12.11.2014}

Figure \ref{fig:hadoop-hdfs} shows how HDFS organizes the data. Here, each file is replicated twice (replication factor of 2). Note how HDFS tries to store block replicas on different nodes. For the sake of simplicity, all files in this example are smaller than the HDFS maximum block size, thus fitting in a single block.

\begin{figure}
  \centering
  \includegraphics[width=130mm,natwidth=749,natheight=372]{hadoop-hdfs.png}
  \caption[HDFS file storage]{HDFS file storage, each file is replicated twice.}
  \label{fig:hadoop-hdfs}
\end{figure}

HDFS provides rack awareness, which allows applications to consider the physical location of a machine when data has to be stored or moved. Rack awareness allows for a variety of advanced features, e.g., it is faster to execute computations on the nodes where the required data is already available instead of first moving the data to another node. This minimizes possibly slow data traffic. Another example is HDFS itself, that uses its rack awareness for its performant replication process, while considering the physical location of the replicas.

\section{YARN}
\label{chap:hadoop:yarn}
YARN is the resource manager of Hadoop. It schedules and satisfies resource requests of applications. It also monitors running applications. Resources are granted in form of resource containers, that consist of CPU, RAM, storage etc.

The functionality of YARN is provided by one ResourceManager and several NodeManagers (similar to HDFS). This architecture offers the needed scalability. Like in HDFS, the ResourceManager is a single point of failure, but solutions for high availability do also exist here in the form of standby ResourceManagers.\footnote{\url{https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html} last access: 13.11.2014}

The ResourceManager has two components, the Scheduler and the ApplicationsManager. The Scheduler allocates resources for running applications but performs no monitoring of running applications. This is the job of the ApplicationsManager. The ApplicationsManager is responsible for accepting new application submissions, negotiating the first container of an application and monitoring of running applications. The monitoring aspect allows the ApplicationsManager to automatically restart failed applications.

The NodeManager (usually one per node) is responsible for the containers, that run on its node. It monitors their resource usage and reports this information back to the ResourceManager.

A typical YARN application consists of three components:

\begin{description}
  \item[Client]: this is the starting point of every YARN application. It submits the application to the ResourceManager, which allocates a free container in the cluster and starts the ApplicationMaster inside this container.
  \item[ApplicationMaster]: the ApplicationMaster (not to confuse with the ApplicationsManager) communicates with the ResourceManager. The ApplicationMaster can request additional containers, for example if it wants to distribute some of its work. It can return already allocated containers, if they are not needed. And it provides information about its current status via a heartbeat. The heartbeat is used by the ApplicationsManager to determine if the application is still alive or needs to be restarted.
  \item[Additional Containers]: the additional containers are not mandatory, but can be useful, as they provide additional resources. The containers can be used to offload work to them, for example in a master-worker scheme, like implemented in Biohadoop. The master runs on the ApplicationMaster and starts several containers. Each container runs a worker.
\end{description}

There is no defined way for data exchange between an ApplicationMaster and its additional containers. If data exchange is a requirement, it must be implemented separately. To see how this is done in Biohadoop, confer to section \ref{chap:impl:communication}.

The automatic restart capabilities of YARN are not extended to additional containers, as there is no general valid solution for their restart. For example, some containers need to maintain state, which must be reflected on a restart. Other containers may work in a stateless fashion. What YARN does, is to provide information about the states of its additional containers to the ApplicationMaster. This way, the ApplicationMaster can implement the restart of failed containers on its own.

YARN runs applications simultaneously, as long as there are enough cluster resources available. Applications are queued if the currently available resources are not sufficient. Figure \ref{fig:hadoop-app} shows an example for the occupation of a Hadoop cluster with one master node and three slave nodes. The NameNode (HDFS) and ResourceManager (YARN) run on the master node, the DataNodes (HDFS) and NodeManagers (YARN) run on the slave nodes and communicate with the corresponding services on the master node. The applications are started by the clients, that request an initial container for their ApplicationMaster from the ResourceManager. After the ApplicationMasters are started they in turn request additional containers from the ResourceManager. In this example, two applications are started. Application 1 occupies five containers and Application 2 occupies seven containers.

\begin{figure}
  \centering
  \includegraphics[width=130mm,natwidth=749,natheight=480]{hadoop-app.png}
  \caption[Occupation of a Hadoop cluster for two applications]{Occupation of a Hadoop cluster for two applications. The NameNode (HDFS) and ResourceManager (YARN) run on the master node. DataNodes (HDFS) and NodeManagers (YARN) run on the slave nodes and communicate with the corresponding services on the master node. The applications, including the ApplicationMasters (AM container), run on the slave nodes inside containers.}
  \label{fig:hadoop-app}
\end{figure}

\section{Oozie}
\label{chap:hadoop:oozie}
Apache Oozie is a workflow tool, that runs on Hadoop. The workflow jobs are Directed Acyclic Graphs (DAGs) of actions, written in XML.\footnote{\url{https://www.w3.org/TR/xml/} last access: 09.01.2015} Oozie uses an internal scheduler to make decisions about the submitted workflows and their progress, the action execution is delegated to Hadoop.

Each Oozie action has two possible outcomes: ``ok'' if the action completed successfully, and ``error'' in the case of an error. The action outcome influences the next transition in the DAG. 

An Oozie workflow consists of two types of nodes, the control flow nodes and the action nodes. Control flow nodes define the sequence of actions in a workflow (start, end, failure, decision, fork, join). Action nodes are used to execute a program or computation.

Oozie provides a number of default actions, like the Java action, that can be used to start a YARN application. Other actions include MapReduce actions, HDFS file system actions (move, delete and mkdir), Email, etc. Oozie allows also to implement new custom actions, that expose different behaviors. In the course of this work a custom action was implemented to invoke Biohadoop from Oozie. More details about this custom action and its configuration can be found in chapter \ref{chap:impl:oozie}.

Figure \ref{fig:oozie-example} shows a workflow example. It is started at the control flow node labeled ``Start'' followed by  the ``FS action''. The transition to the next step is depending on the result of this action. Actions ``Failed'' and ``Fork'' are invoked in case of an error or success, respectively. The ``Fork'' action starts two Java actions in parallel. Actions performed inside a ``Fork'' action have a special error semantic. If an error occurs in any of the parallel actions inside a fork node, all parallel actions are considered as failed. If no error happened, the ``Join'' node waits for two (or possibly more) parallel actions to finish. Then, a ``Decision'' action is invoked, that transitions to the ``MapReduce'' action if the needed conditions are given. If the whole workflow has no errors it terminates at the ``End'' node.

\begin{figure}
  \centering
  \includegraphics[width=100mm,natwidth=631,natheight=512]{oozie-example.png}
  \caption[Oozie workflow transitions]{Oozie workflow transitions. If any action returns with an error, the workflow transitions to the final state ``Failed'', else the workflow advances according to the defined sequence.}
  \label{fig:oozie-example}
\end{figure}

It can be difficult and error prone to manually write a workflow XML. Graphical tools like Hue \footnote{\url{https://gethue.com/category/oozie/} last access: 26.12.2014} can simplify this process.