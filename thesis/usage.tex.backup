\chapter{Using Biohadoop}
\label{chap:usage}
One of the design goals of Biohadoop was to provide a framework for distributed computation on the Hadoop platform, that is easy to use. The result is a simple API, that can be used to implement algorithms. This chapter introduces into the necessary steps to write an algorithm that is capable of being run by Biohadoop on a Hadoop environment. In addition, the presented algorithm uses the capabilities of the task system to distribute parts of its work to Biohadoops workers, to achieve a higher level of parallelism.

\section{Example algorithm}
\label{chap:usage:algorithm}
For the sake of simplicity we reuse the example algorithm \texttt{Sum} of chapter \ref{chap:impl:system-architecture}. As mentioned there, it is a simple algorithm, whose only purpose is to sum the values of an integer array. In listing \ref{listing:sum} we have already a first implementation, that is capable of being run by Biohadoop. But in this example, no usage of the task system and therefor distribution is taken. Also, all the parameters are fixed. In the next sections, we will create step by step an implementation that is configurable using Biohadoops configuration file, and that is able to distribute its work (see section \ref{chap:impl:configuration} for more details about the configuration file).

To make it more transparent how and when we use the task system, we modify the algorithm a little bit. It should use a number of integer arrays (the number is called \texttt{CHUNKS} from here on), each array should be of a fixed size (the size is called \texttt{CHUNK\_SIZE} from here on). If we would append all the chunks one after another, we would have one big integer array of size $\texttt{CHUNCKS} * \texttt{CHUNCK\_SIZE}$. If you look at it from the other side, a big integer array would have to be split into several chunks, to make it suitable for parallel computation. So there is no real difference to the original algorithm, it just preserves us from doing some computations to split a big integer array into smaller arrays. 

Listing \ref{lst:usage-build-data} shows how the data is created. The method \texttt{buildData(int chunks, int chunkSize)} takes \texttt{CHUNCKS} and \texttt{CHUNCK\_SIZE} as input argument and returns \texttt{CHUNKS} integer arrays, each of size \texttt{CHUNCK\_SIZE}. The arrays are filled with consecutive numbers, starting from 0. The consecutive numbers continue between the boundaries of adjacent arrays. For example, array0=[0,1,2], array1=[3,4,5], ...

\lstinputlisting[caption=Building the integer arrays for the \texttt{Sum} algorithm,label=lst:usage-build-data]{../listings/chap-usage-buildData.lst}

  \subsection{Configuring the algorithm}
  \label{chap:usage:configuration}
  Lets begin with modifying the algorithm in a way, such that it can be configured by Biohadoops configuration file. The configuration file is read by Biohadoop at its startup, and the private parameters for an algorithm instance are provided when the \texttt{compute(SolverId solverId, Map<String, String> properties)} method of the algorithm is called. If we use the configuration file of listing \ref{lst:sum-configuration}, we see that we get two properties:
  \begin{itemize}
    \item \texttt{CHUNKS}: the number of integer arrays, that should be produced by the algorithm.
    \item \texttt{CHUNK\_SIZE}: the size of each integer array.
  \end{itemize}
  Those parameters are of type \texttt{String}, so we need to convert them first to integer values, to make them suitable for the \texttt{buildData(int, int)} method, mentioned in \ref{chap:usage:algorithm}. After this steps are taken, we have configured our algorithm through a Biohadoop configuration file, and we have initialized the data. Listing \ref{lst:usage-configured} shows, how the algorithm looks at the current stage. The \texttt{buildData(int, int)} method was skipped, to make the code more concise, it can be found in listing \ref{lst:usage-build-data}.
  
  \lstinputlisting[caption=Configuring the \texttt{Sum} algorithm and initializing the data,label=lst:usage-configured]{../listings/chap-usage-configured.lst}

  \subsection{Parallelization using the task system}
  \label{chap:usage:parallel-algorithm}
  The parallelization of an algorithm using Biohadoop consists of four steps:
  \begin{enumerate}
    \item provide an implementation of \texttt{AsyncComputable}, that can be used to compute the task results by the workers.
    \item create a task submitter, that can be used to add new tasks to the task system.
    \item submit tasks to the task system, by using the before mentioned submitter.
    \item wait for the tasks to complete. It is possible to block during the wait process, but is also possible to do other work in between.
  \end{enumerate}
  
  In step 1 we have to implement \texttt{AsyncComputable}. The resulting class can be used by the workers to compute the results for the tasks. In our case, the workers get as input arguments an integer array and have to compute the sum over all of its elements. Listing \ref{lst:usage-async-computable} shows how this would be done in a class with name \texttt{AsyncSumComputation}. We see that this class is typed, the types have to match the task submitter in step 2. The types are \texttt{Object} for the \texttt{initialData}, but we don't take usage of it in this case. The input data is of type \texttt{int[]}, which matches our integer arrays. As output we have a single integer, which is the result of the summation. The content of the method itself is straight forward.
  
  \lstinputlisting[caption=Summation of all values in an integer array\, done in an texttt{AsyncComputable} class\, that is suitable to be run by workers,label=lst:usage-async-computable]{../listings/chap-usage-asyncComputable.lst}
  
  Step 2 demands the creation of a task submitter, to be able to add tasks to the task system. Lets clarify at this point, what a single task means in our example program: each single task consists of computing the result of an integer array. The data for this task is the given integer array. This data is send to workers, that use a given \texttt{AsynComputable} class to compute the result - in our case the sum of the integer array computed by the \texttt{AsyncSumComputation} class. The result is then returned to the algorithm, at which point the task has completed.
  
  The \texttt{Sum} example algorithm uses many integer arrays, therefor we have many tasks, one task for every integer array. Tasks are submitted to the task system, by submitting their data through a task submitter. The usage of a task submitter is advised, as it hides some complexity from the algorithm author. But it is possible to communicate to the task system without a submitter. We choose to use the task submitter, and create a new one the following way:
  
  \begin{lstlisting}
TaskSubmitter<int[], Integer> taskSubmitter = new SimpleTaskSubmitter<Object, int[], Integer>(AsyncSumComputation.class);
  \end{lstlisting}
  
  This task submitter is configured to get an object of type \texttt{Object} as \texttt{initialData}. As we don't want to use this feature, we don't provide such an \texttt{initialData}. The submitter is further configured to receive tasks of type \texttt{int[]} and to return \texttt{TaskFutures} of type {Integer}, that can be used by the algorithm to retrieve the results from the task system. The tasks of type \texttt{int[]} that we are going to submit are exactly the arrays, that we created in the previous section, using the \texttt{buildData(int, int)}. We store all those arrays in a single 2-dimensional array of type \texttt{int[][]} and call this array \texttt{data}. The only parameter that we provide to the new instance of \texttt{SimpleTaskSubmitter} is the \texttt{AsyncSumComputation} class. This class is used by the workers to compute the results for the tasks, that are submitted through this task submitter.

  In step 3 we start to submit tasks to the task system. As we want to submit a number of arrays, it is best to do it in a loop. We also want to be able to use the results of the different sums to compute the final sum. Therefor we have to store the \texttt{TaskFutures}, to be able to retrieve their results:
  
  \begin{lstlisting}
List<TaskFuture<Integer>> taskFutures = new ArrayList<>();
for (int i = 0; i < chunks; i++) {
  TaskFuture<Integer> future = taskSubmitter.add(data[i]);
  taskFutures.add(future);
}
  \end{lstlisting}
  
  Step 4 consists of waiting for the results. The \texttt{get()} method of a \texttt{TaskFuture} provides us the result, but blocks until a result is available. \texttt{TaskFuture} provides also a \texttt{isDone()} method, which we can use to check for finished computations in a non-blocking way. If \texttt{isDone()} returns true, we can get the results by invoking \texttt{get()}, that doesn't block anymore at that time.
  
  We choose to the simpler \texttt{get()} approach at the cost of blocking the algorithm. As we need all results to proceed, this makes no difference to the non-blocking variant. Because we got several \texttt{TaskFuture} objects from our task submissions in step 3, we have to loop on all the \texttt{TaskFuture} objects to get all of their results. By summing up the results, we get the final sum over all arrays:
  
  \begin{lstlisting}
int sum = 0;
for (TaskFuture<Integer> future : taskFutures) {
  sum += future.get();
}
  \end{lstlisting}
  
  After this step completes, we have the final sum over all arrays, and the algorithm can terminate.
  
  Something that wasn't mentioned until now is, that the task submission and result retrieval operations may throw checked exceptions. This exceptions must be caught, therefor the submission and retrieval must be surrounded by a \texttt{try ... catch} block. In the example code above, this step was omitted to keep the code clear and concise. In the appendix, the whole code can be found, once for the \texttt{Algorithm} (see listing \ref{lst:appendix-sum-full}), and once for the \texttt{AsyncComputable} (see listing \ref{lst:appendix-sum-async}). More examples can be found at \cite{biohadoop-algorithms}.
  
\section{Run Biohadoop}
\label{chap:usage:run}
Although the main purpose of Biohadoop is to be run in a Hadoop environment, it also be run in a local environment. This is for example useful when new algorithms are developed. In this case, the whole process of compilation, deployment to a Hadoop environment and testing can be abbreviated.

To run Biohadoop, three components must be available (four if Biohadoop is started in a Hadoop environment):

\begin{itemize}
  \item An installation of Java in version 1.7 or higher. From here on it is assumed, that Java is installed and configured and that the \texttt{JAVA\_HOME} variable points to this installation.
  \item All necessary libraries must be present and accessible in one or several folders.
  \item A valid configuration file must be present and accessible.
  \item The fourth component is only necessary when running Biohadoop in a Hadoop environment. This component is Hadoop itself, which must be available in a version >= 2.
\end{itemize}

If those three components are provided, Biohadoop can be started. In a local environment, this is done by setting the right class paths when launching the program. In a Hadoop environment, the configuration option \texttt{includePaths} must be set, to include the necessary files (see \ref{chap:impl:configuration} on more information about configuring Biohadoop). Those paths have to point to valid locations of an accessible HDFS file system.

Biohadoop was developed using Maven \cite{maven}. Therefor it is rather easy to get all the needed libraries, as they are declared as dependencies. The source code for Biohadoop can be found at \cite{biohadoop}. By invoking the following command on the projects root folder, all dependencies are accumulated and put to the sub folder \texttt{target/dependency}:

\begin{lstlisting}
mvn dependency:copy-dependencies
\end{lstlisting}

From there, they can be directly referenced through Javas \texttt{classpath} option when running in a local environment. When running in a Hadoop environment, the libraries need to be copied to an accessible HDFS file system, and this location must be present in the before mentioned \texttt{includePaths} configuration option.

For a quick tutorial on how to compile and run Biohadoop from the sources, please refer to appendix \ref{chap:appendix:biohadoop-sources}. To use Biohadoop in a Hadoop environment, such an environment must be present. It can be a difficult task to configure such a Hadoop environment, therefor in appendix \ref{chap:appendix:biohadoop-docker} a simple method can be found, to use a pre-build Hadoop environment. The only dependency this environment has, is Docker \cite{docker}, which is a simple and lightweight runtime for containers, available on all major operating systems.

  \subsection{Local environment}
  \label{chap:usage:local}
  To start Biohadoop in a local environment, the class paths need to be set to include the necessary libraries. The necessary libraries can be obtained by invoking the Maven command, outlined above. As an additional parameter, the \texttt{-Dlocal} option must be provided to Java. This is the only way to tell Biohadoop that it is launched in a local environment. If this parameter is missing, Biohadoop will complain that it can't connect to Hadoop.
  
  Lets assume, that all of the necessary libraries can be found at the location \path{/home/user/biohadoop/libs}, and the configuration file can be found at \path{/home/user/biohadoop/configs/simple-config-json}. Then, Biohadoop can be started in local mode by running the following command:
  
  \begin{lstlisting}
java -Dlocal -cp /home/user/biohadoop/libs/* at.ac.uibk.dps.biohadoop.hadoop.BiohadoopApplicationMaster /home/user/biohadoop/configs/simple-config-json
  \end{lstlisting}

  The main class \path{at.ac.uibk.dps.biohadoop.hadoop.BiohadoopApplicationMaster} starts Biohadoop, which in turn takes care of starting the configured algorithms, adapters and workers. After all of the algorithms have terminated, either because they have finished their computation or because they had errors, Biohadoop shuts down.
  
  When running Biohadoop in the local environment, all workers are started as threads in the same JVM as Biohadoop (in contrast to a Hadoop environment, where the workers are started in separate JVMs on perhaps different machines). This leads to the fact, that the workers have full access to all (static) objects of Biohadoop, for example the \texttt{Environment} object (see \ref{chap:impl:configuration} for a description of this object). But when those workers are started in a Hadoop environment, this access is not given anymore. So, one has to take care to rely only on the objects and properties, that are provided to the used methods.

  \subsection{Hadoop environment}
  \label{chap:usage:hadoop}
  To start Biohadoop in a Hadoop environment (for example in the one provided in appendix \ref{chap:appendix:biohadoop-docker}), all needed libraries and configuration files must be present in the HDFS file system. In addition, Biohadoops jar file must be accessible through the local file system, as it is started directly by Java. The location of the libraries and configuration files in the HDFS file system must be configured in the configuration file, that is provided to Biohadoop on startup.
  
  Lets assume, that all of the necessary libraries can be found at the HDFS location \path{/biohadoop/libs}, the configuration file can be found at the HDFS location \path{/biohadoop/configs/simple-config-json} and that those paths are part of the configuration file that is provided to Biohadoop at startup. Further more, Biohadoops jar file can be found at \path{/home/user/biohadoop/biohadoop.jar}. Then, Biohadoop can be started in Hadoop mode by running the following command:
  
  \begin{lstlisting}
yarn jar /home/user/biohadoop/biohadoop.jar at.ac.uibk.dps.biohadoop.hadoop.BiohadoopClient /biohadoop/configs/simple-config-json
  \end{lstlisting}

  As one can see, now the command \texttt{yarn} is used to launch Biohadoop. This command takes care of setting all the needed Hadoop environment variables, after which it starts the provided main class. The \texttt{yarn} command is part of Hadoop since version 2, and should be available if Hadoop is configured in the right way. If it is not available, please contact the system administrator.
  
  In contrast to running Biohadoop in local mode, we have now a different main class, that is launched. This is due to the fact, that Yarn needs a startup class, from where it loads the main program. By looking at the source code of \texttt{BiohadoopClient}, one will notice that it starts the \texttt{BiohadoopApplicationMaster} behind the curtains (\texttt{BiohadoopApplicationMaster} is the class that is directly started in local mode).
  
  When running Biohadoop in a Hadoop environment, all workers are started in their own containers, which are under the control of Hadoop. The result is, that the workers can not access the (static) objects of Biohadoop and that all communication between Biohadoop and its workers must be done through 
  
  as threads in the same JVM as Biohadoop (in contrast to a Hadoop environment, where the workers are started in separate JVMs on perhaps different machines). This leads to the fact, that the workers have full access to all (static) objects of Biohadoop, for example the \texttt{Environment} object (see \ref{chap:impl:configuration} for a description of this object). But when those workers are started in a Hadoop environment, this access is not given anymore. So, one has to take care to rely only on the objects and properties, that are provided to the used methods.